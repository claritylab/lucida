<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
"http://www.w3.org/TR/html4/strict.dtd">

<html>
  <head>
    <meta name="generator" content="HTML Tidy, see www.w3.org">
    <link rev="made" href="mailto:taku-ku@is.aist-nara.ac.jp">

    <title>YamCha: Yet Another Multipurpose CHunk Annotator</title>
    <link type="text/css" rel="stylesheet" href="yamcha.css">
  </head>

  <body>
    <h1>YamCha: Yet Another Multipurpose CHunk Annotator</h1>

    <p>$Id: index.html,v 1.23 2003/01/06 13:11:21 taku-ku Exp
    $;</p>
    <hr>

    <h2>Introduction</h2>

    <div style="margin-left: 2em">
      <p><b>YamCha</b> is a generic, customizable, and open source
      text chunker oriented toward a lot of NLP tasks, such as POS
      tagging, Named Entity Recognition, base NP chunking, and Text
      Chunking. <b>YamCha</b> is using a state-of-the-art machine
      learning algorithm called Support Vector Machines (SVMs),
      first introduced by Vapnik in 1995.</p>

      <p><b>YamCha</b> is exactly the same system which performed
      the best in the <a href=
      "http://lcg-www.uia.ac.be/conll2000/chunking/">CoNLL2000
      Shared Task, Chunking</a> and <a href=
      "http://lcg-www.uia.ac.be/~erikt/research/np-chunking.html">BaseNP
      Chunking task</a>.</p>
    </div>

    <h2>Table of contents</h2>

    <ul>
      <li><a href="#features">Features</a></li>

      <li><a href="#news">News</a></li>

      <li>
        <a href="#download">Download</a> 

        <ul>
          <li><a href="#source">Source</a></li>

          <li><a href="#rpm">Binary/Source package for RedHat
          Linux</a></li>

          <li><a href="#windows">Binary package for
          MS-Windows</a></li>
        </ul>
      </li>

      <li><a href="#install">Installation</a></li>

      <li>
        <a href="#usage">Usage</a> 

        <ul>
          <li><a href="#format">Training and Test file
          formats</a></li>

          <li><a href="#training">Training and Testing</a></li>

          <li><a href="#tuning">Parameter Tuning</a></li>

          <li><a href="#others">Other options</a></li>
        </ul>
      </li>

      <li><a href="#bib">Bibliography</a></li>

      <li><a href="#ack">Acknowledgments</a></li>

      <li><a href="#links">Links</a></li>
    </ul>

    <h2><a name="features">Features</a></h2>

    <ul>
      <li>Moderately high performance chunker based on Support
      Vector Machines</li>

      <li>Independent from the given task, training/testing with
      any data which can be seen as a "generic" text chunking
      task</li>

      <li>Can redefine feature sets (window-size),
      parsing-direction (forward/backward) and algorithms of
      multi-class problem (pair wise/one vs rest)</li>

      <li>Practical chunking time (1 or 2 sec./sentence. it highly
      depends on the task)</li>

      <li>Can perform partial chunking</li>

      <li>C/C++ library</li>
    </ul>

    <h2><a name="news">News</a></h2>

    <ul>
      <li>
        <strong>2003-01-31</strong>: <a href="#download">yamcha
        0.23</a> Released<br>
        <ul>
          <li>Fix memory leak bugs</li>
	  <li>Use Visual Studio .NET instead of MinGW to build Windows binary
        </ul>
      </li>
    
      <li>
        <strong>2003-01-06</strong>: yamcha
        0.22 Released<br>

        <ul>
          <li>Update param.cpp, yet another command line
          parser</li>
        </ul>
      </li>

      <li>
        <strong>2003-01-01</strong>: yamcha 0.21 Released<br>

        <ul>
          <li>Update <a href="../darts">darts</a></li>
        </ul>
      </li>

      <li>
        <strong>2002-11-11</strong>: yamcha 0.2 Released<br>

        <ul>
          <li>Modify many old-fashioned codes</li>

          <li>Can select strategies for multi-class problem:
          pair-wise or one vs rest</li>

          <li>Save memory when a large model file is generated</li>

          <li>Fix bug about column_size parameter</li>

          <li>Use mmap(3) to make the time for initialization
          faster</li>

          <li>Supports gcc 3.2, Borland C++, and Visual Studio
          .NET</li>

          <li>Change the default sentence boundary marker from
          "EOS" to empty.</li>

          <li>Delete Perl/Ruby modules (I will rewrite them using
          <a href="http://www.swig.org">SWIG</a>)</li>

          <li>Delete -f option, use -V option insted.</li>
        </ul>
      </li>

      <li>
        <strong>2001-7-09</strong>: yamcha 0.1<br>

        <ul>
          <li>Initial Release!</li>
        </ul>
      </li>
    </ul>

    <h2><a name="download">Download</a></h2>

    <ul>
      <li><b>YamCha</b> is free software; you can redistribute it
      and/or modify it under the terms of the <a href=
      "http://www.gnu.org/copyleft/lesser.html">GNU Lesser General
      Public License</a>.</li>

      <li><b>YamCha</b> is distributed in the hope that it will be
      useful, but WITHOUT ANY WARRANTY; without even the implied
      warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR
      PURPOSE. See <a href=
      "http://www.gnu.org/copyleft/lesser.html">GNU Lesser General
      Public License</a>. the for more details.</li>

      <li>
        Please let <a href=
        "mailto:taku-ku@is.aist-nara.ac.jp">me</a> know if you use
        <b>YamCha</b> for research purpose or find any research
        publication where <b>YamCha</b> is applied. 

        <h3><a name="source">Source</a></h3>

        <ul>
          <li>yamcha-0.23.tar.gz: <a href=
          "./src/yamcha-0.23.tar.gz">HTTP</a></li>
        </ul>

        <h3><a name="rpm">Binary/Source package for RedHat
        Linux</a></h3>

        <ul>
          <li>RedHat 7.2 i386: <a href=
          "./redhat-7.x/RPMS/i386/">HTTP</a></li>

          <li>
            RedHat 7.2 SRPMS: <a href=
            "./redhat-7.x/SRPMS/">HTTP</a> 

            <p>Binary packages are built on RedHat 7.2. You must
            re-rebuild packages from SRPMS when you use RedHat 7.3
            or higher.</p>
          </li>
        </ul>

        <h3><a name="windows">Binary package for
        MS-Windows</a></h3>

        <ul>
          <li><a href="./win/">HTTP</a><br>
          Windows Version does not contain the training
          programs.<br>
          Model files generated under 'i386 Linux' can be used in
          MS-Windows version.</li>
        </ul>
      </li>
    </ul>

    <h2><a name="install">Installation</a></h2>

    <ul>
      <li>
        Requirements 

        <ul>
          <li>perl 5.00x or higher</li>

          <li>GNU make</li>

          <li>sort, uniq, rm, cat (they are fundamental UNIX
          tools)</li>

          <li><a href=
          "http://cl.aist-nara.ac.jp/~taku-ku/software/TinySVM/">TinySVM</a></li>

          <li>C++ compiler (gcc 2.95 or higher, does not support
          gcc 2.8 or 2.7)</li>
        </ul>
      </li>

      <li>
        How to make 
<pre>
% ./configure 
% make
% make check
% su
# make install
</pre>
        You can change default install path by using --prefix
        option of configure script.<br>
        Try --help option for finding out other options.
      </li>
    </ul>

    <h2><a name="usage">Usage</a></h2>

    <ul>
      <li>
        <a name="format">Training and Test file formats</a> 

        <p>Both the training file and the test file need to be in a
        particular format for the <b>YamCha</b> to work properly.
        Generally speaking, training and test file must consist of
        multiple <b>tokens</b>. In addition, a <b>token</b>
        consists of multiple (but fixed-numbers) columns. The
        definition of tokens depends on the task, however, in the
        most of typical cases, they simply correspond to the
        <b>words</b>. Each token must be represented in one line,
        with the columns separated by white space (spaces or
        tabular characters). The sequence of token becomes a
        <b>sentence</b>. To identify the boundary between
        sentences, just put an empty line (or just put 'EOS').</p>

        <p>You can give as many columns as you like, however the
        number of columns must be fixed through all tokens.
        Furthermore, there are some kinds of "semantics" among the
        columns. For example, 1st column is 'word', second column
        is 'POS tag' third column is 'sub-category of POS' and so
        on.</p>

        <p>The last column represents a true answer tag which is
        going to be trained by SVMs.</p>

        <p>Here's an example of such a file: (data for CoNLL shared
        task)</p>
<pre>
He        PRP  B-NP
reckons   VBZ  B-VP
the       DT   B-NP
current   JJ   I-NP
account   NN   I-NP
deficit   NN   I-NP
will      MD   B-VP
narrow    VB   I-VP
to        TO   B-PP
only      RB   B-NP
#         #    I-NP
1.8       CD   I-NP
billion   CD   I-NP
in        IN   B-PP
September NNP  B-NP
.         .    O

He        PRP  B-NP
reckons   VBZ  B-VP
..
</pre>

        <p>There are 3 columns for each token.</p>

        <ul>
          <li>The word itself (e.g. reckons);</li>

          <li>part-of-speech associated with the word (e.g.
          VBZ);</li>

          <li>Chunk(Answer) tag represented in IOB2 format;</li>
        </ul>
        <br>
        <br>
         

        <p>The following data is invalid, since the number of
        columns of second and third are 2. (They have no POS
        column.) The number of columns should be fixed.</p>
<pre>
He        PRP  B-NP
reckons   B-VP
the       B-NP
current   JJ   I-NP
account   NN   I-NP
..
</pre>

        <p>Here is an example of English POS-tagging.<br>
        There are total 12 columns; 1: word, 2: contains
        number(Y/N), 3: capitalized(Y/N), 4:contains symbol
        (Y/N)<br>
        5..8 (prefixes from 1 to 4) 9..12 (suffixes from 1 to
        4).<br>
        If there is no entry in a column, dummy field ("__nil__")
        is assigned.</p>
<pre>
Rockwell N Y N R Ro Roc Rock l ll ell well NNP
International N Y N I In Int Inte l al nal onal NNP
Corp. N Y N C Co Cor Corp . p. rp. orp. NNP
's N N N ' 's __nil__ __nil__ s 's __nil__ __nil__ POS
Tulsa N Y N T Tu Tul Tuls a sa lsa ulsa NNP
unit N N N u un uni unit t it nit unit NN
said N N N s sa sai said d id aid said VBD
..
</pre>
      </li>

      <li>
        <a name="training">Training and Testing</a> 

        <p>The first step in using the <b>YamCha</b> is to create
        training and test files. Here, I take the <a href=
        "http://lcg-www.uia.ac.be/~erikt/research/np-chunking.html">
        Base NP Chunking</a> task as a case study.</p>

        <p>Assume a data set like <a href="train.data">this</a>.
        First column represents a word. Second column represents a
        POS tag associated with the word. Third column is true
        answer tag associated with the word (I,O or B). The chunks
        are represented using IOB2 model. The sentences are
        presumed to be separated by one blank line.</p>

        <p>First of all, run <b>yamcha-config</b> with
        <b>--libexecdir</b> option. The location of Makefile which
        is used for training is output. Please copy the Makefile to
        the local working directory.</p>
<pre>
% yamcha-config --libexecdir
/usr/local/libexec/yamcha
% cp /usr/local/libexec/yamcha/Makefile .
</pre>

        <p>There are two mandatory parameters for training.</p>

        <ul>
          <li><b>CORPUS</b>: The location of file which is written
          in the <a href="#format">training/test</a> format.</li>

          <li><b>MODEL</b>: Prefix name of model file(s)</li>
        </ul>

        <p>Here is an example in which <b>CORPUS</b> is set as
        'train.data' and <b>MODEL</b> is set as 'case_study'.</p>
<pre>
% make CORPUS=train.data MODEL=case_study train
/usr/bin/yamcha  -F'F:-2..2:0.. T:-2..-1' &lt; train.data &gt; case_study.data
perl -w /usr/local/libexec/yamcha/mkparam   case_study &lt; case_study.data
perl -w /usr/local/libexec/yamcha/mksvmdata case_study
.. omit
</pre>

        <p>After training, the following files are generated.</p>
<pre>
% ls case_study.*
case_study.log           : log of training
case_study.model         : model file (binary, architecture dependent)
case_study.txtmodel.gz   : model file (text, architecture independent)
case_study.se            : support examples
case_study.svmdata       : training data for SVMs
</pre>

        <p>OK, let's parse this <a href="train.data">test data</a>
        using above generated model file (case_study.model). You
        simply use the command:</p>
<pre>
% yamcha -m case_study.model &lt; test.data 
Rockwell        NNP     B       B
International   NNP     I       I
Corp.   NNP     I       I
's      POS     B       B
Tulsa   NNP     I       I
unit    NN      I       I
said    VBD     O       O
...
</pre>

        <p>The last column is given (estimated) tag. If the 3rd
        column is true answer tag , you can evaluate the accuracy
        by simply seeing the difference between the 3rd and 4th
        columns.</p>
      </li>

      <li>
        <a name="tuning">Parameter Tuning</a> 

        <ul>
          <li>
            <b>Parsing Direction</b> 

            <p><b>DIRECTION</b> is used to change the parsing
            direction. The default setting is forward parsing mode
            (LEFT to RIGHT). If "-B" is specified, backward parsing
            mode (RIGHT to LEFT) is used. Please see my paper for
            more detail about the parsing direction.</p>
<pre>
% make CORPUS=train.data MODEL=case_study DIRECTION="-B" train   
</pre>
          </li>

          <li>
            <b>Re-definition of features (changing window-size)</b>
            

            <p><b>FEATURE</b> is used to change the feature sets
            (window-size) for chunking.<br>
             The default setting is <b>"F:-2..2:0..
            T:-2..-1"</b>.</p>

            <p><b>"F:-2..2:0.. T:-2..-1"</b> implies that contexts
            in the blue box are used as feature sets to identify
            the tag in the red box.</p>

            <p><img src="./feature.png" alt="features"></p>

            <p>More specifically, the contexts in the blue box can
            be divided into two parts -- green box (static feature
            F:) and light-blue box (dynamic feature T:).<br>
             F: and T: should be written in the following
            format:</p>
<pre>
F:[beginning pos. of token]..[end pos. of token]:[beginning pos. of column]..[end pos. of column]
T:[beginning pos. of tag]..[end pos. of tag]
</pre>

            <p><b>Static Features F:</b><br>
             In this figure, the tokens at -2, -1, 0, 1, and 2
            position are used as features. (green box).<br>
             It means that [beginning positing of token] is
            <b>-2</b> and [end position of token] is <b>+2</b>.<br>
             In addition, this figure shows that 0-th and 1-st
            columns in these tokens are taken as features.<br>
             It means that [beginning position of column] is
            <b>0</b> and [the end position of column] is
            <b>1</b>.<br>
             You can omit the [end position of column]. If omitted,
            the last column is set as [end position of column].<br>
             Note that column for answer tag is not regarded as
            [end position of column].<br>
             By taking tokens as well as columns, final expression
            of static feature becomes <b>"F:-2..2:0..1"</b>.<br>
             In this case, you can use <b>"F:-2..2:0.."</b> which
            means same as <b>"F:-2..2:0..1"</b>.</p>

            <p><b>Dynamic Features T:</b><br>
             Dynamic features are decided dynamically during the
            tagging of chunk labels.<br>
             In this figure, the tags at -2 and -1 position are
            used as features. (light-blue box)<br>
             It means that [beginning positing of tag] is <b>-2</b>
            and [end position of tag] is <b>-1</b>.<br>
             Note that [end potion of tag] must smaller than -1,
            since the right-side tags (0,+1,+2,+3...)<br>
             have not been identified yet and cannot be used as
            features.</p>

            <p>You can use the expression F: and T: repeatably. All
            duplicate entries are deleted.</p>

            <p>Here are more complicated examples.</p>

            <p></p>

            <table border="1">
              <tr>
                <td align="center" width="350"><img src=
                "./feature2.png"><br>
                 <b>F:-3..3:0.. T:-3..-1</b></td>

                <td align="center" width="350"><img src=
                "./feature3.png"><br>
                 <b>F:-2..2:1..1 F:0..0:0..1 T:-1..-1</b></td>
              </tr>

              <tr>
                <td align="center" width="350"><img src=
                "./feature4.png"><br>
                 <b>F:-3..-2:0.. F:0..0:0.. F:2..3:0..
                T:-3..-2</b></td>

                <td align="center" width="350"><img src=
                "./feature5.png"><br>
                 <b>F:-3..-2:1..1 F:-1..0:0..0 F:2..3:1..1
                T:-3..-1</b></td>
              </tr>
            </table>

            <p>Here is an example of setting <b>"F:-3..3:0..
            T:-3..-1"</b> to the <b>FEATURE</b> parameter.</p>
<pre>
% make CORPUS=train.data MODEL=case_study FEATURE="F:-3..3:0.. T:-3..-1" train
</pre>

            <p>The expression <b>"-2..2"</b> can be also expressed
            as <b>"-2,-1,0,-1,2"</b>. In addition, if the beginning
            position and end position are same, you can omit the
            end position. Here are some alternative
            expressions:</p>

            <ul>
              <li><b>"F:-2..2:0..0"</b> -&gt;
              <b>"F:-2,-1,0,1,2:0"</b></li>

              <li><b>"F:0..0:0..1"</b> -&gt; <b>"F:0:0,1"</b></li>
            </ul>

            <p>Note that the expression of <b>"-2,0,2"</b> is
            different from <b>"-2..2"</b>.<br>
             ".." represents a range between beginning and end
            position.</p>
          </li>

          <li>
            <b>Call-back function to rewrite features in detail
            (require C++ knowledge)</b> 

            <p>You can define some call-back function which
            re-writes or adds task-dependent specific features. For
            more detail, see example/example.cpp.</p>
          </li>

          <li>
            <b>Multi-class methods</b> 

            <p><b>MULTI_CLASS</b> is used to change the strategy
            for the multi-class problem. The default setting is
            <b>pair wise</b> method. If "2" is specified, 'one vs
            rest' is used.</p>
<pre>
% make CORPUS=train.data MULTI_CLASS=2 MODEL=case_study
</pre>
          </li>

          <li>
            <b>Training conditions of SVMs</b> 

            <p><b>SVM_PARAM</b> is used to change the training
            condition of SVMs. Default setting is <b>"-t1 -d2
            -c1"</b>, which means the 2nd degree of polynomial
            kernel and 1 slack variable are used. Note that
            <b>YamCha</b> only supports <b>polynomial</b>
            kernels.</p>
            Here is an example of using 3rd degree of polynomial
            kernel: 
<pre>
% make CORPUS=train.data MODEL=case_study SVM_PARAM="-t1 -d3 -c1" train
</pre>

            <p>Please use -m SIZE option to increase the memory for
            training if possible. This option drastically reduce
            the computational cost and time.<br>
             Here is an example of assigning 512 Mb memory to the
            SVMs:</p>
<pre>
% make CORPUS=train.data MODEL=case_study SVM_PARAM="-t1 -d2 -c1 -m 512" train
</pre>
          </li>

          <li>
            <b>Output format</b> 

            <p>The <b>-V</b> option sets verbose mode, where yamcha
            outputs tag and scores of all candidates.<br>
            The meaning of <b>score</b> varies with multi-class
            methods.</p>

            <ul>
              <li>one vs rest: distance from the separating
              hyperplane</li>

              <li>pair wise: summation of distances of this
              class</li>
            </ul>
            <br>
            <br>
             
<pre>
# without -V
% yamcha  -m case_study.model &lt; test.data
Rockwell        NNP     B       B
International   NNP     I       I
Corp.   NNP     I       I
's      POS     B       B
Tulsa   NNP     I       I
unit    NN      I       I
said    VBD     O       O
..

# with -V
% yamcha -V -m case_study.model
Rockwell        NNP     B       B       B/0.630616      I/-0.974367     O/-0.721942
International   NNP     I       I       B/-0.789851     I/0.561522      O/-0.833703
Corp.   NNP     I       I       B/-0.934675     I/0.486497      O/-0.584086
's      POS     B       B       B/0.418284      I/-0.760627     O/-0.794485
Tulsa   NNP     I       I       B/-0.987653     I/1.06272       O/-1.16405
unit    NN      I       I       B/-0.783824     I/0.845213      O/-1.04919
said    VBD     O       O       B/-1.29512      I/-1.02006      O/0.956885
...
</pre>
          </li>

          <li>
            <b>Sentence boundary marker</b> 

            <p>The <b>-e</b> option sets the sentence boundary
            marker. Default setting is empty ("").<br>
            Here is an example of changing the sentence boundary
            marker to "EOS"</p>
<pre>
% yamcha -e EOS -m case_study.model &lt; test.data
</pre>
            <br>
          </li>

          <li>
            <b>Partial Chunking</b> 

            <p>If you know in advance the candidates of answer tags
            by using some 'prior' knowledge, you may want to select
            answer only from these candidates. Here is a concrete
            example. If the 1st token must be B tag and the 2nd
            token must be select only from B and I, you give yamcha
            the following test data:</p>
<pre>
Rockwell        NNP     B
International   NNP     B      I
</pre>

            <p>Generally speaking, candidates are listed instead of
            last column.<br>
            In the partial parsing mode, yamcha must be run with -C
            option.</p>
<pre>
% yamcha -C -m case_study.model &lt; test.data
</pre>

            <p>Note that the interpretation of test data varies
            with -C option.</p>

            <ul>
              <li>With -C option: the last (or more) columns are
              interpreted as candidates.</li>

              <li>Without -C option: the last (or more) columns are
              ignored.</li>
            </ul>
            <br>
            <br>
          </li>
        </ul>
      </li>

      <li>
        <a name="others">Other options</a> 

        <p>See <a href="./yamcha.html">here</a>.</p>
      </li>
    </ul>

    <h2><a name="bib">Bibliography</a></h2>

    <ul>
      <li>
        Publications in which <b>YamCha</b> is applied 

        <ul>
          <li>Taku Kudo, Yuji Matsumoto (2001)<br>
           Chunking with Support Vector Machines, NAACL 2001 [<a
          href=
          "http://cl.aist-nara.ac.jp/~taku-ku/publications/naacl2001.pdf">
          PDF</a>]</li>

          <li>Taku Kudo, Yuji Matsumoto (2000)<br>
           Use of Support Vector Learning for Chunk Identification,
          CoNLL-2000 [<a href=
          "http://cl.aist-nara.ac.jp/~taku-ku/publications/conll2000.ps">
          PS</a>]</li>

          <li>Hiroyasu Yamada, Taku Kudo, Yuji Matsumoto (2002)<br>
           Japanese Named Entity Extraction using Support Vector
          Machine'', Transactions of IPSJ, Vol. 43, No. 1, pages
          44-53, 2002. (in Japanese)</li>

          <li>Tatsumi Yoshida and Kiyonori Ohtake and Kazuhide
          Yamamoto (2002)<br>
           Comparative Experiments of Chinese Analyzers between
          Support Vector Machines and Minimum Connective Costs
          Method, IPSJ SIG NL-150 (in Japanese) [<a href=
          "http://www.slt.atr.co.jp/~kohtake/publishing/SIG-NL-150.pdf">
          PDF</a>]</li>

          <li>Koichi Takeuchi and Nigel Collier (2002)<br>
           Use of support vector machines in extended named entity,
          CoNLL-2002</li>
        </ul>
      </li>

      <li>
        Others 

        <ul>
          <li>Vladimir N. Vapnik (1995)<br>
           The Nature of Statistical Learning Theory, Springer</li>
        </ul>
      </li>
    </ul>

    <h2><a name="ack">Acknowledgments</a></h2>

    <div style="margin-left: 2em">
      <p>I would like to appreciate all the people that were
      involved in the development of this software: the members in
      <a href="http://cl.aist-nara.ac.jp/">Computational
      Linguistics Laboratory</a> at <a href=
      "http://www.aist-nara.ac.jp">NAIST</a>, and also to
      particular individuals:</p>

      <ul>
        <li><a href=
        "http://www.slt.atr.co.jp/~kohtake/index.html">Kiyonori
        OHTAKE</a> who gives me a number of patches to fix
        bugs.</li>

        <li>Kaoru Yamamoto who reviews this manual.</li>

        <li><a href=
        "http://cl.aist-nara.ac.jp/staff/matsu/home-e.html">Yuji
        MATSUMOTO</a> who is my supervisor, introduces me to the
        world of statistical NLP.</li>
      </ul>
    </div>

    <h2><a name="links">Links</a></h2>

    <ul>
      <li><a href=
      "http://lcg-www.uia.ac.be/~erikt/research/np-chunking.html">NP
      chunking</a></li>

      <li><a href=
      "http://lcg-www.uia.ac.be/conll2000/chunking/">chunking</a></li>

      <li><a href=
      "http://cl.aist-nara.ac.jp/~taku-ku/software/TinySVM/">TinySVM</a>,
      my developing SVMs-toolkit</li>

      <li><a href=
      "http://cl.aist-nara.ac.jp/~taku-ku/software/cabocha/">CaboCha</a>,
      my developing SVMs-based Japanese Dependency Analyzer (in
      Japanese)</li>
    </ul>
    <hr>

    <p>$Id: index.html,v 1.23 2003/01/06 13:11:21 taku-ku Exp
    $;</p>

    <address>
      taku-ku@is.aist-nara.ac.jp
    </address>
  </body>
</html>

